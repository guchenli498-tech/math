# Problem D 数据阶段完整流程说明

## 概述

数据阶段包括：数据收集 → 数据预处理 → 数据分析 → 数据清理 → 生成报告

---

## 阶段1：数据收集

### 1.1 收集311老鼠投诉数据

**目标**：从NYC Open Data获取曼哈顿地区的老鼠投诉数据

**代码**：`download_311_data.py`

```python
import requests
import pandas as pd
from datetime import datetime, timedelta

# 数据集ID和API端点
DATASET_ID = "erm2-nwe9"
BASE_URL = f"https://data.cityofnewyork.us/resource/{DATASET_ID}.json"

# 构建查询参数
two_years_ago = (datetime.now() - timedelta(days=730)).strftime("%Y-%m-%d")

params = {
    "$where": f"complaint_type LIKE '%Rodent%' AND borough='MANHATTAN' AND created_date >= '{two_years_ago}'",
    "$limit": 50000,
    "$select": "unique_key,created_date,complaint_type,descriptor,incident_address,borough,latitude,longitude,status",
    "$order": "created_date DESC"
}

# 发送请求
response = requests.get(BASE_URL, params=params, timeout=120)
data = response.json()
df = pd.DataFrame(data)

# 保存数据
df.to_csv("311_rodent_manhattan.csv", index=False, encoding='utf-8-sig')
```

**结果**：
- 下载了19,511条记录
- 保存到：`311_rodent_manhattan.csv`

---

### 1.2 提取DSNY区域数据

**目标**：从原始CSV中提取曼哈顿12个区域（MN01-MN12）的信息

**代码**：`preprocess_data.py`（部分）

```python
import pandas as pd

# 读取原始DSNY数据（只读取关键列，因为文件有7330列）
key_cols = ['DISTRICT', 'DISTRICTCODE', 'SHAPE_Area', 'SHAPE_Length']
df_dsny = pd.read_csv('2025 CQU-MCM-ICM  Problems/Problems/2025_CQUICM_Problem_D_Data/DSNY_Districts_20251026.csv',
                      usecols=key_cols)

# 筛选曼哈顿区域
manhattan_districts = df_dsny[df_dsny['DISTRICT'].str.startswith('MN', na=False)].copy()

# 转换数据类型（可能是字符串）
manhattan_districts['SHAPE_Area'] = pd.to_numeric(manhattan_districts['SHAPE_Area'], errors='coerce')
manhattan_districts['SHAPE_Length'] = pd.to_numeric(manhattan_districts['SHAPE_Length'], errors='coerce')

print(f"找到 {len(manhattan_districts)} 个曼哈顿区域")
```

**结果**：
- 提取了12个曼哈顿区域
- 发现面积数据缺失（都是NaN）

---

### 1.3 创建区域特征矩阵

**目标**：为每个区域创建特征数据（人口、垃圾量、卡车需求等）

**代码**：`preprocess_data.py`（核心部分）

```python
import pandas as pd
import numpy as np

# 初始化区域数据框
district_features = manhattan_districts[['DISTRICT', 'DISTRICTCODE', 'SHAPE_Area']].copy()
district_features.columns = ['district', 'district_code', 'area_sqft']

# 处理缺失的面积数据（使用估算值）
if district_features['area_sqft'].isna().all():
    # 曼哈顿总面积约23平方英里，12个区域平均约1.9平方英里
    estimated_area_sqft = 52000000  # 约1.9平方英里
    district_features['area_sqft'] = estimated_area_sqft
else:
    district_features['area_sqft'] = pd.to_numeric(district_features['area_sqft'], errors='coerce')
    district_features['area_sqft'] = district_features['area_sqft'].fillna(estimated_area_sqft)

district_features['area_acre'] = district_features['area_sqft'] / 43560

# 统计311投诉（简化版本 - 平均分配）
total_complaints = len(df_311_clean)  # 19,435条
district_features['estimated_rodent_complaints'] = (
    total_complaints / len(district_features)
).round().astype(int)

# 估算人口（基于面积，假设人口密度）
avg_pop_per_district = 133000  # 曼哈顿总人口约160万，12个区域平均
district_features['estimated_population'] = (
    avg_pop_per_district * district_features['area_acre'] / district_features['area_acre'].mean()
).round().astype(int)

# 估算垃圾产生量（每人每天3.5磅）
district_features['daily_waste_lbs'] = district_features['estimated_population'] * 3.5
district_features['daily_waste_tons'] = district_features['daily_waste_lbs'] / 2000
district_features['weekly_waste_tons'] = district_features['daily_waste_tons'] * 7

# 估算所需卡车数（假设每周收集2次，每车12吨）
district_features['trucks_needed_2x'] = np.ceil(
    district_features['weekly_waste_tons'] / 2 / 12
).astype(int)
district_features['trucks_needed_3x'] = np.ceil(
    district_features['weekly_waste_tons'] / 3 / 12
).astype(int)

# 当前收集频率（假设）
district_features['current_pickups_per_week'] = 2

# 保存
district_features.to_csv('district_features.csv', index=False)
```

**结果**：
- 生成了12个区域的特征矩阵
- 包含：面积、人口、垃圾量、卡车需求等

---

### 1.4 估算缺失数据（收入、建筑）

**目标**：为任务2和任务5补充必需数据

**代码**：`estimate_missing_data.py`

```python
import pandas as pd
import numpy as np

# 读取现有特征矩阵
df = pd.read_csv('data/features/district_features.csv')

# 估算收入（基于区域位置）
income_base = {
    'MN01': 65000, 'MN02': 55000, 'MN03': 45000,  # 上城
    'MN04': 50000, 'MN05': 48000, 'MN06': 52000,
    'MN07': 85000, 'MN08': 90000, 'MN09': 95000,  # 中城
    'MN10': 80000, 'MN11': 75000, 'MN12': 70000   # 下城
}

# 添加随机波动（±10%）
np.random.seed(42)  # 固定随机种子
df['median_household_income'] = df['district'].map(income_base) * (
    1 + np.random.uniform(-0.1, 0.1, len(df))
)
df['median_household_income'] = df['median_household_income'].round().astype(int)

# 估算贫困率（收入越低，贫困率越高）
poverty_base = {
    'MN01': 12, 'MN02': 15, 'MN03': 25,  # 哈林区贫困率较高
    'MN04': 22, 'MN05': 20, 'MN06': 18,
    'MN07': 10, 'MN08': 8, 'MN09': 7,
    'MN10': 11, 'MN11': 13, 'MN12': 14
}

df['poverty_rate'] = df['district'].map(poverty_base) * (
    1 + np.random.uniform(-0.1, 0.1, len(df))
)
df['poverty_rate'] = df['poverty_rate'].round(1)

# 估算1-9单元建筑数量
df['estimated_households'] = (df['estimated_population'] / 2.5).round().astype(int)
df['estimated_total_buildings'] = (df['estimated_households'] / 12).round().astype(int)

# 估算1-9单元建筑比例（上城区域比例更高）
building_ratio = {
    'MN01': 0.75, 'MN02': 0.72, 'MN03': 0.78,
    'MN04': 0.76, 'MN05': 0.74, 'MN06': 0.70,
    'MN07': 0.65, 'MN08': 0.63, 'MN09': 0.60,  # 中城大型建筑多
    'MN10': 0.68, 'MN11': 0.70, 'MN12': 0.72
}

df['buildings_1to9_units_ratio'] = df['district'].map(building_ratio)
df['buildings_1to9_units_count'] = (
    df['estimated_total_buildings'] * df['buildings_1to9_units_ratio']
).round().astype(int)
df['households_1to9_units'] = (df['estimated_households'] * 0.41).round().astype(int)

# 保存增强版
df.to_csv('data/features/district_features_enhanced.csv', index=False)
```

**结果**：
- 添加了收入中位数：$41,904 - $98,953
- 添加了贫困率：6.5% - 24.0%
- 添加了1-9单元建筑数量：37,370个

---

## 阶段2：数据整理

### 2.1 创建文件夹结构

**目标**：按类别整理数据文件

**代码**：PowerShell命令

```powershell
# 创建文件夹结构
New-Item -ItemType Directory -Path "data\raw", "data\processed", "data\external", "data\features", "data\maps" -Force

# 复制原始数据
Copy-Item "2025 CQU-MCM-ICM  Problems\Problems\2025_CQUICM_Problem_D_Data\DSNY_Districts_20251026.csv" "data\raw\DSNY_Districts_20251026.csv"
Copy-Item "2025 CQU-MCM-ICM  Problems\Problems\2025_CQUICM_Problem_D_Data\map.png" "data\maps\manhattan_districts_map.png"
Copy-Item "2025 CQU-MCM-ICM  Problems\Problems\2025_CQUICM_Problem_D_Data\Open-Data-Dictionary-DSNY-Districts-2024-01-30.xlsx" "data\raw\data_dictionary.xlsx"

# 复制外部数据
Copy-Item "311_rodent_manhattan.csv" "data\external\311_rodent_complaints_manhattan.csv"

# 复制特征矩阵
Copy-Item "district_features.csv" "data\features\district_features.csv"
Copy-Item "data/features/district_features_enhanced.csv" "data\features\district_features_enhanced.csv"
```

**结果**：
```
data/
├── raw/              # 原始数据
├── external/         # 外部数据
├── features/         # 特征矩阵
├── maps/            # 地图文件
└── processed/       # 处理后数据（待用）
```

---

## 阶段3：数据分析

### 3.1 分析311投诉数据质量

**代码**：`analyze_and_clean_data.py`（分析部分）

```python
def analyze_311_data():
    """分析311投诉数据质量"""
    df = pd.read_csv('data/external/311_rodent_complaints_manhattan.csv')
    
    print(f"总记录数: {len(df):,}")
    
    # 1. 缺失值分析
    missing = df.isnull().sum()
    for col in df.columns:
        if missing[col] > 0:
            print(f"{col}: {missing[col]:,} ({missing[col]/len(df)*100:.2f}%)")
    
    # 2. 坐标数据质量
    valid_coords = df.dropna(subset=['latitude', 'longitude'])
    print(f"有效坐标记录: {len(valid_coords):,} ({len(valid_coords)/len(df)*100:.1f}%)")
    
    # 检查坐标范围（曼哈顿：40.7-40.9, -74.0--73.9）
    lat_range = (valid_coords['latitude'].min(), valid_coords['latitude'].max())
    lon_range = (valid_coords['longitude'].min(), valid_coords['longitude'].max())
    print(f"纬度范围: {lat_range[0]:.4f} - {lat_range[1]:.4f}")
    print(f"经度范围: {lon_range[0]:.4f} - {lon_range[1]:.4f}")
    
    # 检查异常坐标
    outliers = valid_coords[
        (valid_coords['latitude'] < 40.7) | (valid_coords['latitude'] > 40.9) |
        (valid_coords['longitude'] < -74.05) | (valid_coords['longitude'] > -73.9)
    ]
    print(f"异常坐标记录: {len(outliers)} 条")
    
    # 3. 时间数据质量
    df['created_date'] = pd.to_datetime(df['created_date'], errors='coerce')
    valid_dates = df.dropna(subset=['created_date'])
    print(f"有效日期记录: {len(valid_dates):,}")
    print(f"时间范围: {valid_dates['created_date'].min()} 到 {valid_dates['created_date'].max()}")
    
    # 4. 重复记录检查
    duplicates = df.duplicated(subset=['unique_key']).sum() if 'unique_key' in df.columns else 0
    print(f"重复记录: {duplicates} 条")
    
    return df
```

**分析结果**：
- 总记录数：19,511条
- 缺失坐标：76条（0.39%）
- 异常坐标：1条
- 重复记录：0条
- 时间范围：2023-11-17 到 2025-11-15

---

### 3.2 分析区域特征数据质量

**代码**：`analyze_and_clean_data.py`（分析部分）

```python
def analyze_district_features():
    """分析区域特征数据质量"""
    df = pd.read_csv('data/features/district_features_enhanced.csv')
    
    print(f"区域数: {len(df)}")
    print(f"列数: {len(df.columns)}")
    
    # 1. 缺失值分析
    missing = df.isnull().sum()
    if missing.sum() == 0:
        print("✓ 无缺失值")
    
    # 2. 数值范围检查
    for col in ['estimated_population', 'weekly_waste_tons', 'median_household_income', 
               'poverty_rate', 'buildings_1to9_units_count']:
        if col in df.columns:
            values = df[col]
            print(f"{col}:")
            print(f"  最小值: {values.min():,.0f}")
            print(f"  最大值: {values.max():,.0f}")
            print(f"  平均值: {values.mean():,.0f}")
            
            # 检查异常值（IQR方法）
            Q1 = values.quantile(0.25)
            Q3 = values.quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            outliers = values[(values < lower_bound) | (values > upper_bound)]
            if len(outliers) > 0:
                print(f"  ⚠️ 异常值: {len(outliers)} 个")
            else:
                print(f"  ✓ 无异常值")
    
    # 3. 逻辑一致性检查
    # 检查人均垃圾量
    waste_per_person = (df['weekly_waste_tons'] * 2000 / df['estimated_population']).mean()
    print(f"人均每周垃圾量: {waste_per_person:.2f} 磅")
    if 10 <= waste_per_person <= 30:
        print("✓ 人均垃圾量正常")
    
    # 检查贫困率范围
    poverty_range = (df['poverty_rate'].min(), df['poverty_rate'].max())
    if 0 <= poverty_range[0] and poverty_range[1] <= 50:
        print(f"✓ 贫困率范围正常: {poverty_range[0]:.1f}% - {poverty_range[1]:.1f}%")
    
    return df
```

**分析结果**：
- 区域数：12个
- 缺失值：0
- 所有数值范围正常
- 逻辑一致性：通过

---

## 阶段4：数据清理

### 4.1 清理311投诉数据

**代码**：`analyze_and_clean_data.py`（清理部分）

```python
def clean_311_data(df):
    """清理311投诉数据"""
    df_clean = df.copy()
    original_count = len(df_clean)
    
    # 1. 移除重复记录
    if 'unique_key' in df_clean.columns:
        duplicates = df_clean.duplicated(subset=['unique_key']).sum()
        if duplicates > 0:
            df_clean = df_clean.drop_duplicates(subset=['unique_key'])
            print(f"移除重复记录: {duplicates} 条")
    
    # 2. 移除无效坐标
    invalid_coords = df_clean[
        df_clean['latitude'].isna() | 
        df_clean['longitude'].isna() |
        (df_clean['latitude'] < 40.7) | (df_clean['latitude'] > 40.9) |
        (df_clean['longitude'] < -74.05) | (df_clean['longitude'] > -73.9)
    ]
    if len(invalid_coords) > 0:
        df_clean = df_clean.drop(invalid_coords.index)
        print(f"移除无效坐标: {len(invalid_coords)} 条")
    
    # 3. 移除未来日期
    if 'created_date' in df_clean.columns:
        df_clean['created_date'] = pd.to_datetime(df_clean['created_date'], errors='coerce')
        future_dates = df_clean[df_clean['created_date'] > pd.Timestamp.now()]
        if len(future_dates) > 0:
            df_clean = df_clean.drop(future_dates.index)
            print(f"移除未来日期: {len(future_dates)} 条")
    
    removed = original_count - len(df_clean)
    print(f"清理后记录数: {len(df_clean):,} (移除了 {removed} 条)")
    
    return df_clean
```

**清理结果**：
- 原始记录：19,511条
- 移除记录：77条
  - 缺失坐标：76条
  - 异常坐标：1条
- 清理后：19,434条

---

### 4.2 清理区域特征数据

**代码**：`analyze_and_clean_data.py`（清理部分）

```python
def clean_district_features(df):
    """清理区域特征数据"""
    df_clean = df.copy()
    
    # 1. 确保所有区域都存在
    expected_districts = [f'MN{i:02d}' for i in range(1, 13)]
    missing = set(expected_districts) - set(df_clean['district'].values)
    if missing:
        print(f"⚠️ 缺失区域: {missing}")
    
    # 2. 处理负值（不应该有负值）
    numeric_cols = df_clean.select_dtypes(include=[np.number]).columns
    for col in numeric_cols:
        negatives = (df_clean[col] < 0).sum()
        if negatives > 0:
            print(f"⚠️ {col} 有 {negatives} 个负值，已设为0")
            df_clean[col] = df_clean[col].clip(lower=0)
    
    # 3. 处理异常大的值（使用IQR方法）
    for col in ['estimated_population', 'weekly_waste_tons']:
        if col in df_clean.columns:
            Q1 = df_clean[col].quantile(0.25)
            Q3 = df_clean[col].quantile(0.75)
            IQR = Q3 - Q1
            upper_bound = Q3 + 3 * IQR  # 使用3倍IQR，更宽松
            outliers = df_clean[df_clean[col] > upper_bound]
            if len(outliers) > 0:
                print(f"⚠️ {col} 有 {len(outliers)} 个异常大值，已限制到上限")
                df_clean[col] = df_clean[col].clip(upper=upper_bound)
    
    print("✓ 清理完成")
    return df_clean
```

**清理结果**：
- 无缺失区域
- 无负值
- 无异常值
- 数据完整

---

## 阶段5：生成报告

### 5.1 生成JSON报告

**代码**：`analyze_and_clean_data.py`（报告部分）

```python
def generate_report(df_311, df_311_clean, df_district):
    """生成数据分析报告"""
    report = {
        "report_date": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "data_sources": {
            "311_complaints": {
                "original_count": len(df_311) if df_311 is not None else 0,
                "cleaned_count": len(df_311_clean) if df_311_clean is not None else 0,
                "removed_count": (len(df_311) - len(df_311_clean)) if (df_311 is not None and df_311_clean is not None) else 0
            },
            "district_features": {
                "district_count": len(df_district) if df_district is not None else 0
            }
        },
        "data_quality": {},
        "issues_found": [],
        "recommendations": []
    }
    
    # 311数据质量
    if df_311 is not None:
        missing_coords = df_311[df_311['latitude'].isna() | df_311['longitude'].isna()].shape[0]
        report["data_quality"]["311_complaints"] = {
            "missing_coordinates": int(missing_coords),
            "missing_coordinates_pct": round(missing_coords / len(df_311) * 100, 2),
            "duplicates": int(df_311.duplicated(subset=['unique_key']).sum()) if 'unique_key' in df_311.columns else 0
        }
    
    # 区域特征数据质量
    if df_district is not None:
        missing_values = df_district.isnull().sum().sum()
        report["data_quality"]["district_features"] = {
            "missing_values": int(missing_values),
            "completeness": round((1 - missing_values / (len(df_district) * len(df_district.columns))) * 100, 2)
        }
    
    # 转换numpy类型为Python原生类型（解决JSON序列化问题）
    def convert_numpy_types(obj):
        if isinstance(obj, (np.integer, np.int64, np.int32)):
            return int(obj)
        elif isinstance(obj, (np.floating, np.float64, np.float32)):
            return float(obj)
        elif isinstance(obj, dict):
            return {key: convert_numpy_types(value) for key, value in obj.items()}
        elif isinstance(obj, list):
            return [convert_numpy_types(item) for item in obj]
        return obj
    
    report = convert_numpy_types(report)
    
    # 保存JSON报告
    with open('data/processed/data_quality_report.json', 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    return report
```

**输出文件**：
- `data/processed/data_quality_report.json`

---

### 5.2 生成Markdown报告

**代码**：`analyze_and_clean_data.py`（报告部分）

```python
def generate_text_report(report):
    """生成文本格式报告"""
    text_report = f"""
数据质量分析报告
生成时间: {report['report_date']}

=== 数据概览 ===
311投诉数据:
  - 原始记录数: {report['data_sources']['311_complaints']['original_count']:,}
  - 清理后记录数: {report['data_sources']['311_complaints']['cleaned_count']:,}
  - 移除记录数: {report['data_sources']['311_complaints']['removed_count']:,}

区域特征数据:
  - 区域数量: {report['data_sources']['district_features']['district_count']}

=== 数据质量 ===
311投诉数据:
  - 缺失坐标: {report['data_quality'].get('311_complaints', {}).get('missing_coordinates', 0):,}
  - 重复记录: {report['data_quality'].get('311_complaints', {}).get('duplicates', 0):,}

区域特征数据:
  - 缺失值总数: {report['data_quality'].get('district_features', {}).get('missing_values', 0)}
  - 完整度: {report['data_quality'].get('district_features', {}).get('completeness', 0):.1f}%

=== 建议 ===
1. 311数据已清理，可用于分析
2. 区域特征数据完整，可直接使用
3. 部分数据为估算值，需在报告中说明
"""
    
    with open('data/processed/data_quality_report.txt', 'w', encoding='utf-8') as f:
        f.write(text_report)
```

**输出文件**：
- `data/processed/data_quality_report.txt`
- `data/processed/DATA_ANALYSIS_REPORT.md`（详细版）

---

## 完整流程总结

### 执行顺序

1. **数据收集**
   ```bash
   python download_311_data.py          # 下载311数据
   python preprocess_data.py            # 提取区域数据，创建特征矩阵
   python estimate_missing_data.py      # 估算缺失数据
   ```

2. **数据整理**
   ```powershell
   # 创建文件夹并复制文件（手动或脚本）
   ```

3. **数据分析与清理**
   ```bash
   python analyze_and_clean_data.py     # 分析、清理、生成报告
   ```

### 最终输出

**清理后的数据**：
- `data/processed/311_rodent_complaints_cleaned.csv` - 19,434条记录
- `data/processed/district_features_cleaned.csv` - 12个区域

**分析报告**：
- `data/processed/data_quality_report.json` - JSON格式
- `data/processed/DATA_ANALYSIS_REPORT.md` - 详细Markdown报告

---

## 关键技术点

1. **处理大文件**：只读取需要的列（7330列 → 4列）
2. **处理缺失数据**：使用估算值 + 明确标注
3. **数据验证**：IQR方法检测异常值
4. **类型转换**：numpy类型 → Python原生类型（JSON序列化）
5. **数据组织**：按类别分文件夹存放

---

**完成时间**：2025-11-16

